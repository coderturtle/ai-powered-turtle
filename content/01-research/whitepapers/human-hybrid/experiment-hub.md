## Title: Experiment Hub â€” Unified Index for Hybrid & AI Tool Experiments  
Author: Head of Architecture & CTO (EU)  
Audience: Experiment Leads, AI Enablement Team, Product & Engineering Heads  
Version: v1.0  
Tags: [experimentation, ai-tools, hybrid-teams, governance, metrics, comparison, baseline]

# ğŸ§­ Purpose

The **Experiment Hub** serves as the single point of coordination and visibility for all experimentation activity across the organisation. It connects both the **Hybrid Experimentation Framework** (focused on Team, Clients, and Process) and the **AI Tool Experimentation Framework** (focused on tool adoption, value, and learning) under one unified structure.

It provides:

- A single, **governed process** for proposing and tracking experiments.
    
- Consistent **baseline and comparison standards** across all experiments.
    
- Clear visibility into **whatâ€™s being tested, whoâ€™s involved, and whatâ€™s working.**
    
- Aggregated metrics and insights for leadership to make data-driven decisions.
    

---

# ğŸ§© Frameworks Overview

|Framework|Focus|Key Question|Output|
|---|---|---|---|
|**Hybrid Experimentation**|Team, Clients, Process|How do hybrid human+agent teams deliver faster, safer, smarter?|New role archetypes, process playbooks, validated delivery models|
|**AI Tool Experimentation**|Bedrock, Copilot, Claude Code, MS Copilot, Writer|Which tools create measurable value, and for whom?|Tool usage patterns, performance data, adoption insights|

---

# ğŸ§± Repository Structure

```bash
/experiments-hub/
â”œâ”€â”€ /hybrid-experiments/              # Team, Client, and Process experiments
â”‚   â”œâ”€â”€ baselines.csv
â”‚   â”œâ”€â”€ dashboard.csv
â”‚   â”œâ”€â”€ playbook.md                   # Hybrid Experimentation Playbook
â”‚   â””â”€â”€ /<experiment-folders>/
â”œâ”€â”€ /ai-tool-experiments/             # Tool-focused experiments
â”‚   â”œâ”€â”€ baselines.csv
â”‚   â”œâ”€â”€ dashboard.csv
â”‚   â”œâ”€â”€ playbook.md                   # AI Tool Experimentation Playbook
â”‚   â””â”€â”€ /<tool-folders>/
â”œâ”€â”€ /shared-assets/
â”‚   â”œâ”€â”€ metric-dictionary.md          # Standardised KPI definitions
â”‚   â”œâ”€â”€ governance-policy.md          # Experiment guardrails & compliance
â”‚   â”œâ”€â”€ templates/                    # Markdown templates for proposals, retros, surveys
â”‚   â”œâ”€â”€ dashboards/                   # Power BI / Streamlit dashboards
â”‚   â””â”€â”€ logo.svg                      # Visual branding asset for Experiment Hub
â””â”€â”€ README.md                         # Entry point linking all playbooks and dashboards
```

---

# ğŸ“ˆ Shared Standards

### 1. Baseline Capture

- All experiments (hybrid or tool) must define **pre-intervention baselines**.
    
- Baseline format: CSV with consistent headers â†’ `experiment_id, pillar/tool, metric, baseline_value, date_collected, data_source`.
    
- Data stored centrally in `/baselines.csv` for both frameworks.
    

### 2. KPI Dictionary (maintained in `/shared-assets/metric-dictionary.md`)

Defines common metrics and formulas:

- **Lead Time** â€“ Avg. time from idea â†’ deployment.
    
- **Cycle Efficiency** â€“ Active time Ã· total time.
    
- **Automation Coverage** â€“ % of tasks auto-completed by agents.
    
- **Client Feedback Loop** â€“ Days from release â†’ first client feedback.
    
- **AI Suggestion Acceptance** â€“ Accepted Ã· total suggestions.
    

### 3. Comparison Methodology

- Express results as **Î”% vs baseline**.
    
- Use shared **Composite Score Formula**: `(Team Ã— 0.4) + (Clients Ã— 0.3) + (Process Ã— 0.3)` for hybrid experiments; `(Productivity Ã— 0.4) + (Quality Ã— 0.3) + (Adoption Ã— 0.2) + (Compliance Ã— 0.1)` for AI tool experiments.
    
- Store in `/dashboard.csv` for aggregation and visualisation.
    

---

# ğŸš€ Governance & Lifecycle

|Stage|Description|Owner|
|---|---|---|
|**Proposal**|Submit via GitHub issue using shared template.|Experiment Lead|
|**Review**|AI Enablement Team validates scope, safety, and metric alignment.|AI Enablement Team|
|**Execution**|Team runs experiment following baseline & data collection rules.|Experiment Owner|
|**Evaluation**|Compare outcomes to baseline, document evidence pack.|Evidence Lead|
|**Publication**|Add results to dashboards and Experiment Library.|AI Enablement PM|
|**Promotion / Scale**|Convert successful patterns into official playbooks.|HOC / Architecture Council|

---

# ğŸ§® Dashboards

Visual dashboards combine data from both frameworks:

- **Hybrid Dashboard:** Delivery velocity, team size vs performance, client NPS improvement.
    
- **AI Tool Dashboard:** Productivity lift, code quality delta, adoption curves, trust scores.
    
- **Aggregate Dashboard:** Composite metrics, experiment health, cross-team trends.
    

Example fields:

|Experiment|Framework|Î” Metric|Score|Owner|Status|
|---|---|---|---|---|---|
|H1 â€“ Hybrid Pod|Hybrid|+22% lead time|0.74|EU Platform|ğŸŸ¢ Active|
|T4 â€“ Copilot BRDs|AI Tool|+18% productivity|0.67|PMO|ğŸ”µ Completed|

---

# ğŸ§­ Roles & Responsibilities

|Role|Description|
|---|---|
|**Experiment Lead**|Owns setup, data collection, and retrospective.|
|**Evidence Lead**|Validates baseline and ensures consistent metric definitions.|
|**Agent Wrangler / Scenario Engineer**|Manages AI agent usage and logs.|
|**AI Enablement PM**|Oversees dashboards, publishes quarterly summary.|
|**Architecture Council**|Reviews learnings, updates frameworks, and scales patterns.|

---

# ğŸ§© Shared Experiment Library Template

```markdown
## Experiment Summary
- Title:
- Framework: (Hybrid / AI Tool)
- Owner(s):
- Start / End Dates:
- Tools Used:
- Pillars / Categories:
- Baseline Summary:
- Results Summary (Î”% vs baseline):
- Composite Score:
- Key Learnings:
- Next Steps:
- Artefacts: (Links to repo, dashboards, reports)
```

---

# ğŸ§­ Success Metrics for 2026â€“2027

- â‰¥25 total experiments executed across both frameworks.
    
- â‰¥80% with validated baselines and comparable metrics.
    
- Quarterly Experiment Hub report to ExCo summarising learnings.
    
- 5+ patterns promoted to enterprise playbooks.
    
- Consistent participation from all regions (EU, US, APAC).
    

---

# ğŸŒ Narrative Anchor

> _â€œOur Experiment Hub is the nervous system of transformation â€” connecting data, people, and ideas.  
> Every experiment tells a story; together, they reveal the architecture of our future.  
> With shared baselines, common measures, and transparent learnings, we move from intuition to evidence, from isolated pilots to an adaptive, AI-powered organisation.â€_