
## Title: Hybrid Experimentation Playbook ‚Äî Framework for Future Team, Client, and Process Experiments  
Author: Head of Architecture & CTO (EU)  
Audience: Experiment Leads, Product & Technology Teams, AI Enablement Council  
Version: v1.2 (adds Experiment Categories: Workflow & Stack ¬∑ Role Evolution ¬∑ Team Topology)  
Tags: [experimentation, ai, team-design, process, client-engagement, categories, workflow-stack, role-evolution, team-topology]

# üß≠ Purpose

This playbook provides a **reusable framework** for designing, running, and evaluating experiments that explore how **AI‚Äëaugmented teams, client engagement, and processes** evolve within the organisation. It now introduces **Experiment Categories** so we can distinguish the _type of change under test_ from the _areas of impact_, enabling clearer comparison and scaling decisions.

- **Experiment Categories (type):**
    
    - **Workflow & Stack** ‚Äî process, pipelines, automation, tools
        
    - **Role Evolution** ‚Äî new/changed responsibilities validated via multiple workflow trials
        
    - **Team Topology** ‚Äî changes to pod size/shape & cross‚Äëfunctional mix
        
- **Impact Dimensions (results):** **Team ¬∑ Clients ¬∑ Process** (reported as deltas vs. baseline)
    

Think of each experiment as a **matrix**: _Category (row)_ √ó _Impact Dimensions (columns)_.


# ‚öôÔ∏è How to Use This Playbook

1. Duplicate this markdown into your team‚Äôs repo/workspace.
    
2. Choose an **Experiment Category** and fill the **Experiment Template** (Section 2).
    
3. Establish a **baseline** before launching any experiment.
    
4. Report results against the three **Impact Dimensions** (Team/Clients/Process).
    
5. Tag your experiment with **category** and context for cross‚Äëcomparison.
    
6. Submit proposals via the **Hybrid Operating Council (HOC)** or AI Enablement Team.
    

---

# 1Ô∏è‚É£ Framework Overview (Impact Dimensions)

|Pillar|Core Belief|Desired Outcome|
|---|---|---|
|**Team**|Leaner is better; roles broaden; new specialists emerge; teams own focused domains.|Identify optimal team size, role mix, and new skill archetypes.|
|**Clients**|Client zeal fuels discovery; early engagement with real software.|Shorten feedback loops; validate value through real client use.|
|**Process**|Crews are agent leaders; automate sustain work; minimise hand‚Äëoffs.|Increase autonomy, velocity, and quality via lean, test‚Äëdriven, AI‚Äëpowered delivery.|

---

# üß± 1A. Experiment Categories (Type)

## ‚úÖ Category definitions

- **Workflow & Stack** ‚Äî Experiments that change _how work flows_ and _which tools/automation_ run in the pipeline (e.g., Copilot/Claude usage, Bedrock guardrails, CI/CD/evals, policy‚Äëas‚Äëcode).
    
- **Role Evolution** ‚Äî Experiments that **bundle ‚â•2 Workflow & Stack trials** to validate a **role change** (e.g., introducing _Scenario Engineer_ or _Agent Wrangler_ responsibilities).
    
- **Team Topology** ‚Äî Experiments that adjust **pod shape** (size, cross‚Äëfunctional mix, guild support) and governance, often incorporating Role Evolution **and** Workflow & Stack changes.
    

## ‚õ≥ Gating logic

- To run **Role Evolution**, reference results from **‚â•2 Workflow & Stack** experiments.
    
- To run **Team Topology**, reference results from **‚â•1 Role Evolution** **and** **‚â•2 Workflow & Stack** experiments.
    

## üè∑Ô∏è Labels & tagging

- GitHub labels: `cat:workflow-stack` ¬∑ `cat:role-evolution` ¬∑ `cat:team-topology`
    
- Dashboard `category` column for slicing comparisons.
    

---

# 2Ô∏è‚É£ Experiment Template (Copy & Fill)

```markdown
## Experiment Title

### 1. Experiment Category (pick one)
- [ ] Workflow & Stack
- [ ] Role Evolution
- [ ] Team Topology

### 2. Baseline
Summarise pre‚Äëexperiment metrics (lead time, satisfaction, rework %, client feedback cycles). Attach `baselines.csv` rows.

### 3. Problem Statement
What organisational or delivery challenge are you addressing?

### 4. Hypothesis (cross‚Äëpillar)
Link Team, Client, and Process assumptions.

### 5. AI Tools & Enablers
| Tool | Role | Purpose |
|------|------|---------|
| Amazon Bedrock | Architect / Agent Wrangler | Multi‚Äëagent workflows, guardrails |
| Claude Code | Engineer / QA | Reasoning refactors, tests |
| GitHub Copilot | Engineer / QA | Code, docs, task automation |
| Microsoft Copilot | PM / Ops / Finance | BRDs, planning, analysis |
| Writer | Comms / Compliance | Policy‚Äësafe content |

### 6. Participants & Roles
List participants and assign roles (Problem Framer, Builder‚ÄëCurator, Agent Wrangler, Scenario Engineer, Evidence Lead, etc.)

### 7. Metrics & Evidence (per Impact Dimension)
| Pillar | Quantitative Metrics | Qualitative Measures |
|--------|----------------------|---------------------|
| Team | Lead time, rework %, engagement score | Role clarity, collaboration |
| Clients | Time‚Äëto‚Äëfirst‚Äëfeedback, NPS, conversion | Perceived client understanding |
| Process | Automation %, dependency count | Team autonomy, process simplicity |

### 8. Evaluation vs Baseline
Present deltas (% improvement/regression) for each pillar with context.

### 9. Cross‚ÄëExperiment Tags
Category, toolset, team size, function, domain, region, experiment type.

### 10. Learning Objectives
What do we expect to learn about future roles, tooling, or governance?

### 11. Scale / Sunset Criteria
Quantitative gates for next phase (e.g., ‚Äú20% cycle‚Äëtime improvement; 0 critical incidents‚Äù).
```

---

# 3Ô∏è‚É£ Baseline & Comparison Framework

## Baseline Principles

- Capture measurable **starting data** before changing roles, processes, or introducing new AI tooling.
    
- Use identical **KPI definitions** across experiments (see Metric Dictionary).
    
- Record context (team size, LOB, toolset, region, **category**) in a **Baseline Register** (`/hybrid-experiments/baselines.csv`).
    
- Collect data for at least **two sprints / four weeks** where feasible.
    

### Example Baseline Metrics

|Pillar|Measurement|Frequency|Data Source|
|---|---|---|---|
|Team|Lead time, rework %, throughput|Weekly|Jira, GitHub, DORA reports|
|Clients|NPS, feedback loop length, client engagement rate|Biweekly|Product analytics, surveys|
|Process|Automation %, dependency count, change‚Äëfail rate|Weekly|CI/CD telemetry|

## Comparison Principles

1. **Normalise** outcomes as **Œî% vs baseline**.
    
2. **Tag** each experiment with **category** + context to enable slicing.
    
3. Use a shared **KPI dictionary** for consistency.
    
4. **Composite scoring** (suggested weights): Team 40% ¬∑ Clients 30% ¬∑ Process 30%.
    
5. Store comparative rows in `/hybrid-experiments/dashboard.csv`.
    

### Comparison Dashboard Schema

`experiment_id, owner, framework, category, metric, delta_pct, composite_score, tags, last_updated`

### Example Comparison View

|Experiment|Category|Team Œî Lead Time|Clients Œî NPS|Process Œî Automation|Composite|
|---|---|---|---|---|---|
|H1 ‚Äì 4‚ÄëPerson Hybrid Pod|Team Topology|‚àí22%|+10|+18%|0.74|
|H7 ‚Äì Scenario Engineer Trial|Role Evolution|‚àí12% rework|+2|+15% eval pass|0.70|
|H3 ‚Äì Feature‚ÄëFlagged CD|Workflow & Stack|‚àí25%|+4|+25%|0.73|

---

# 4Ô∏è‚É£ Pillar Design Checklists

(Updated with baseline and category requirements)

## üß© Team

-  **Category selected** and labels applied.
    
-  Baseline metrics captured (lead time, rework %, engagement).
    
-  Small cross‚Äëfunctional team (‚â§6) where possible.
    
-  Mix of generalists + specialists (Agent Wrangler, Scenario Engineer).
    
-  Post‚Äëexperiment deltas recorded and normalised.
    

## ü§ù Clients

-  Baseline client metrics (NPS, feedback loop).
    
-  Real client feedback < 4 weeks into experiment.
    
-  Client‚Äëfacing artefacts produced with policy‚Äësafe tools (Writer/MS Copilot).
    
-  Comparative deltas captured.
    

## ‚öôÔ∏è Process

-  Baseline cycle time & automation rate captured.
    
-  CI/CD includes eval harness + evidence logging.
    
-  Policy‚Äëas‚Äëcode guardrails enforced (PII, logging).
    
-  Comparative improvement logged post‚Äëexperiment.
    

---

# 5Ô∏è‚É£ Governance & Evidence

|Deliverable|Description|
|---|---|
|**Baseline Register**|Shared repository of pre‚Äëexperiment metrics (with **category** column).|
|**Evidence Pack**|Includes baseline vs outcome deltas + qualitative insights.|
|**Comparison Dashboard**|Aggregates results across experiments; slice by **category**.|
|**Metric Dictionary**|Canonical KPI definitions maintained by AI Enablement Team.|
|**Category Gating Check**|Confirmation that Role Evolution/Team Topology reference prerequisite Workflow & Stack results.|

---

# 6Ô∏è‚É£ Repo Structure (with Categories)

```bash
/hybrid-experiments/
‚îú‚îÄ‚îÄ baselines.csv              # Includes `category` column
‚îú‚îÄ‚îÄ dashboard.csv              # Comparison dashboard (category-sliced)
‚îú‚îÄ‚îÄ /team-experiments/
‚îÇ   ‚îú‚îÄ‚îÄ H1-4-Person-Pod/
‚îÇ   ‚îî‚îÄ‚îÄ playbook.md
‚îú‚îÄ‚îÄ /client-experiments/
‚îÇ   ‚îú‚îÄ‚îÄ H2-Advisor-Workflow/
‚îÇ   ‚îî‚îÄ‚îÄ playbook.md
‚îî‚îÄ‚îÄ /process-experiments/
    ‚îú‚îÄ‚îÄ H3-Continuous-Delivery/
    ‚îî‚îÄ‚îÄ playbook.md
```

---

# 7Ô∏è‚É£ Success Measures (2026‚Äì2027)

- ‚â•10 hybrid experiments with valid baselines.
    
- ‚â•80% use consistent KPI definitions enabling cross‚Äëcomparison.
    
- Documented **25%+ average improvement** across at least two pillars.
    
- **Category coverage:** ‚â•4 Workflow & Stack, ‚â•3 Role Evolution, ‚â•3 Team Topology.
    
- Quarterly **Hybrid Org Experimentation Dashboard** maintained and reviewed.
    

---

# üß© Narrative Anchor

> _‚ÄúEach experiment has a **type** and a **result**.  
> We test **Workflow & Stack**, then prove **Role Evolution**, and finally shape **Team Topology** ‚Äî always reporting impact on **Team, Clients, and Process**.  
> This ladder turns local wins into an org‚Äëwide operating model, backed by baselines and comparable evidence.‚Äù_